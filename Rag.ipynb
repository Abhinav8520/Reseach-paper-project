{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78285d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pdfplumber spacy tqdm pandas transformers sentence-transformers faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d0f972c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MULTI-SCALECROPPINGMECHANISMFORREMOTESENSINGIMAGECAPTIONING\n",
      "XuetingZhang1,QiWang1,ShangdongChen2,XuelongLi1*\n",
      "1SchoolofComputerScienceandCenterforOPTicalIMageryAnalysisandLearning(OPTIMAL),\n",
      "NorthwesternPolytechnicalUniversity,Xi’an710072,Shaanxi,P.R.China.\n",
      "2SchoolofInformationScienceandTechnology,\n",
      "NorthwestUniversity,Xi’an710072,Shaanxi,P.R.China.\n",
      "ABSTRACT Imagecaptioning[3]isacomprehensivetaskwhichcom-\n",
      "binescomputervisionandnaturallanguageprocessing.Since\n",
      "Withtherapiddevelopmentofartificialsatellite,alargenum-\n",
      "that encoder-decoder based method [4] can automaticallty\n",
      "ber of high resolution remote sensing images can be easily\n",
      "learn the hign-level semantic features and dig their textual\n",
      "obtained now. Recently, remote sensing image captioning,\n",
      "relationships, it has dominated the field of image caption-\n",
      "whichaimstogenerateaccurateandconcisedescriptivesen-\n",
      "ing with the best perpormance. The encoder process aims\n",
      "tences for remote sensing images, has been promoted by\n",
      "to represent an image with a feature map/vector by using\n",
      "template-based model and encoder-decoder model with sev-\n",
      "Convolutional Neural Networks (CNNs), while in decoder\n",
      "eral related datasets released. Based on an encoder-decoder\n",
      "process the feature map/vector is decoded into a sentence\n",
      "model,weproposeatrainingmechanismofmulti-scalecrop-\n",
      "by a sequence model, such as Recurrent Neural Networks\n",
      "pingforremotesensingimagecaptioninginthispaper,which\n",
      "(RNNs) and Long-Short Term Memory networks (LSTM)\n",
      "canextractmorefine-grainedinformationfromremotesens-\n",
      "[5]. This type of methods usually learn a large amount of\n",
      "ingimagesandenhancethegeneralizationperformanceofthe\n",
      "embeddedspaceforimagesandcaptions. Becausethereare\n",
      "basemodel. TheexperimentalresultsontwodatasetsUCM-\n",
      "no strict grammatical constraints, the system can generate\n",
      "captionsandSydney-captionsdemonstratethattheproposed\n",
      "relativelynewtextualdescriptionsforinputimages.\n",
      "approach availably improves the performances in describing\n",
      "highresolutionremotesensingimages. Correspondingly, some researches recently have been\n",
      "studied in remote sensing image captioning. Qu et al. [6]\n",
      "IndexTerms— Remotesensingimage,imagecaptioning,\n",
      "firstly propose a deep multi-modal neural network model to\n",
      "encoder-decoder,multi-scalecropping\n",
      "solvetheproblemofunderstandingHSRremotesensingim-\n",
      "agesatthesemanticlevel,thenShietal.[7]presentaremote\n",
      "1. INTRODUCTION sensingimagecaptioningframeworkbyleveragingtherecent\n",
      "techniquesofdeeplearningandfullyconvolutionalnetworks.\n",
      "Benefited from the rapid development of deep learning, re- For the former, different CNN architectures with RNN or\n",
      "cently, many researches in the field of remote sensing have LSTM are combined to generate meaningful sentences for\n",
      "beengreatlypromoted,includingobjectdetection,sceneclas- remotesensingimages.Whileforthelatter,atemplate-based\n",
      "sification [1] [2], semantic segmentation and so on. These model is applied to generate concise descriptions. Further-\n",
      "tasksmainlyexploretheattributesofvisualfeaturesfromthe more, based on a multimodal model, attention mechanism\n",
      "remote sensing images such as category, width, height and is introduced by Lu et al [8]. And both the handcrafted and\n",
      "others.Forthetaskofremotesensingimagecaptioning,how- deepfeaturesareutilizedinthismethod.\n",
      "ever, it has to further dig the relationship between these at-\n",
      "However,therearemanyproblemsneedtobesolved.For\n",
      "tributesanddescribeitwithflexiblesentencesinhumanlan-\n",
      "example, it is quite significant to explore how to reduce the\n",
      "guage. Tomakethemostoftheinformationinremotesens-\n",
      "overfitting problem of the model with the limit of training\n",
      "ingimages,remotesensingimagecaptioningisabletoextract\n",
      "samples. Inthefieldofimageclassification,tencropsmech-\n",
      "boththevisualandthecontextfeaturesfromanimage,which\n",
      "anism [9] is introduced to reduce the overfitting problem in\n",
      "canhelppeopleintuitivelyunderstandthecontentofremote\n",
      "thetrainingstage,whichisaneffectivemethodfordataargu-\n",
      "sensingimagesonasemanticlevel.Ithasawiderangeofap-\n",
      "mentation. Besides,multi-scaletraining[10]isacommonly\n",
      "plicationprospectsinmanydomainssuchasimageretrieval,\n",
      "used method in the field of object detection. Inspired by\n",
      "resourceinvestigation,disasterdetectionandmilitaryintelli-\n",
      "thesemethods,weproposeamulti-scalecroppingmechanism\n",
      "gencegeneration.\n",
      "forremotesensingimagecaptiongeneration, combiningten\n",
      "∗CorrespondingAuthor. cropswithmulti-scaletraining. Theoverallframeworkwith\n",
      "978-1-5386-9154-0/19/$31.00 ©2019 IEEE 10039 IGARSS 2019\n",
      "Authorized licensed use limited to: University of Florida. Downloaded on March 27,2024 at 23:22:37 UTC from IEEE Xplore. Restrictions apply.\n",
      "scale 1 resize\n",
      "Randomly Airplanes with\n",
      "scale 2 resize selected LSTM different size are\n",
      "CNN stopped at the\n",
      "airport.\n",
      ".. . .. . Sentence\n",
      "Image\n",
      "scale n resize\n",
      "Feature Vector\n",
      "Fig.1.Theframeworkoftheproposedmethod.\n",
      "theproposedmethodisshowninFig. 1. Themaincontribu-\n",
      "tionsofthispaperaresummarizedasfollows: d(cid:48) =d∗s (1)\n",
      "1. A significant training mechanism of multi-scale crop- In this paper, d is 256 and S is set as [1.0, 0.875, 0.66].\n",
      "ping for remote sensing image caption generation is Thus the range of d(cid:48) is [256, 224, 169]. Obviously, d(cid:48) is no\n",
      "proposed in this paper. The proposed training mecha- larger than d. Motivated by Tencrop, the patch is randomly\n",
      "nismcanimprovetheperformanceofimagecaptioning cropped from the five corners (upper left, lower left, upper\n",
      "withtheeffectofreducingtheoverfittingproblem. right,lowerrightandthecenter)andtheflippedfivecorners\n",
      "of the images. For each image, there are 22 (10 x 2 + 2)\n",
      "2. Basedontheencoder-decoderframework,wetrainand\n",
      "possiblecroppingpaths.Thenthecroppedpatchwithvarious\n",
      "testdifferentcombinedmodelsofCNNsandLSTMon\n",
      "sizeisresizedto224x224beforebeingsenttotheencoder-\n",
      "twodatasetsUCM-captionsandSydney-captions.\n",
      "decoder model. Thus the cropping mechanism can provide\n",
      "3. Theabundantexpermentialresultsprovetheeffective- variouscroppedpatchesP i (i=1,...,10)fromeachtraining\n",
      "ness of the proposed apporach for remote sensing im- image,whichcanimprovethegeneralizationofthemodel.\n",
      "agecaptioning.\n",
      "2.2. Encoder-DecoderFramework\n",
      "2. METHODOLOGY\n",
      "Togeneratemorenovelsentencesforremotesensingimages,\n",
      "ourapproachismainlybasedonthepopularencoder-decoder\n",
      "TheoverviewofwholeframeworkisshowninFig.1.Andthe\n",
      "framework,whichisdividedintotwostages:imagerepresen-\n",
      "structureoftheproposedmethodmainlycontainstwoparts:\n",
      "tation and sentence generation. In more detail, the encoder\n",
      "a multi-scale cropping mechanism and an encoder-decoder\n",
      "process encodes an image into a fixed-length feature vector,\n",
      "basedframework.Beforebeinginputintoanencoder-decoder\n",
      "while the decoder process aims to decode the feature vector\n",
      "framework,thegivenimagesneedtobeprocessedbyamulti-\n",
      "intoameaningfulsentence.\n",
      "scalecroppingmechanism.\n",
      "2.1. Multi-scaleCropping 2.2.1. ImageReresentation\n",
      "Noting that the effective feature extraction is critical to the With the explosive development of deep learning and com-\n",
      "task of remote sensing image captioning, we introduce a puter vision, extracting features from images by learning\n",
      "multi-scalecroppingtrainingmechanismtoimprovethegen- based method has gradually gained popularity [11]. Due to\n",
      "eralizationoffeaturerepresentation. theexcellentabilityofautomaticallyextractingadvancedfea-\n",
      "AsshowninFig.1,theinputoftheframeworkisanimage tureswithfewerparameters,ConvolutionalNeuralNetworks\n",
      "thatisresizedtodxd. Weneedtoselectanpatchofd(cid:48) xd(cid:48) (CNNs) are introduced here for image representation. By\n",
      "fromtheimagethroughthemethodofmulti-scalecropping. replacingthelastfullyconnectedlayer,threedeepCNNsare\n",
      "Firstly,scalesisrandomlypickedupfromascalelistS that usedtorepresentthecroppedimageswithafixed-lengthvec-\n",
      "issetmanuallyinadvance. AndalltheelementsofS areno tor,whicharepre-trainedonImageNetdataset. Considering\n",
      "largerthan1.0. Themathematicalrelationshipofd,d(cid:48) ands theimagespreprocessedbythemulti-scalecroppingmecha-\n",
      "is: nism have been randomly cropped into different sizes, both\n",
      "10040\n",
      "Authorized licensed use limited to: University of Florida. Downloaded on March 27,2024 at 23:22:37 UTC from IEEE Xplore. Restrictions apply.\n",
      "Table 1. PERFORMANCES OF THE MULTI-SCALE Table 2. PERFORMANCES OF THE MULTI-SCALE\n",
      "CROPPING MECHANISM FOR REMOTE SENSING IM- CROPPING MECHANIASM FOR REMOTE SENSING\n",
      "AGECAPTIONINGONTHEUCM-CAPTIONSDATASET. IMAGE CAPTIONING ON THE SYDNEY-CAPTIONS\n",
      "B-NISBLEUSCOREFORN-GRAM. DATASET.B-NISBLEUSCOREFORN-GRAM.\n",
      "Model Scale B-1 B-2 B-3 B-4 Model Scale B-1 B-2 B-3 B-4\n",
      "[s ] 57.1 50.5 44.6 38.3 [s ] 54.9 45.0 39.3 31.9\n",
      "1 1\n",
      "VGG-16 [s ,s ] 58.0 50.7 45.2 39.5 VGG-16 [s ,s ] 56.3 48.1 41.9 33.7\n",
      "1 2 1 2\n",
      "[s 1,s 2,s 3] 59.4 51.4 46.3 41.6 [s 1,s 2,s 3] 57.6 49.4 42.5 35.8\n",
      "[s 1] 54.5 46.9 41.8 36.4 [s 1] 58.2 51.6 45.5 39.4\n",
      "Inception-ResNetV2 [s 1,s 2] 54.8 48.5 43.3 37.8 Inception-ResNetV2 [s 1,s 2] 59.7 52.3 46.9 41.7\n",
      "[s 1,s 2,s 3] 56.7 49.7 44.2 38.8 [s 1,s 2,s 3] 60.9 53.4 47.3 41.7\n",
      "[s 1] 58.7 52.3 47.1 42.1 [s 1] 58.8 51.5 44.7 38.2\n",
      "ResNet-152 [s 1,s 2] 59.1 52.6 47.1 42.4 ResNet-152 [s 1,s 2] 60.5 52.3 45.1 38.3\n",
      "[s 1,s 2,s 3] 59.4 53.2 48.1 42.9 [s 1,s 2,s 3] 61.5 54.0 47.3 40.0\n",
      "theglobalandlocalfeaturescanbeobtainedbytheCNN. 3. EXPERIMENTS\n",
      "v =CNN(P ) (2)\n",
      "0 i In this section, we first give an introduction to the datasets\n",
      "Asshowninformula(2),eachpatchP istransferedinto and the metrics for experiments. Then some details of the\n",
      "i\n",
      "CNN, and then a fixed-length feature vector v is extracted experimentssetuparegiven. Finally,theperformanceofex-\n",
      "0\n",
      "fromit. perimentswithdifferentCNNsontwodatasetsarecompared\n",
      "toverifythegeneralizationcapabilitiesofourmethod.\n",
      "2.2.2. SentencesGeneration\n",
      "3.1. DatasetsandMetrics\n",
      "To generate accurate descriptive sentences for remote sens-\n",
      "ingimages,Long-ShortTermMemorynetworks (LSTM)[5] In experiments, two datasets of remote sensing image cap-\n",
      "isappliedinthecaptioningdecodingstage.Byusinggatesto tioning, UCM-captions and Sydney-captions, are used to\n",
      "controlthetransmissionofnetworkinformation,LSTMisde- evaluate the performance of different architectures with the\n",
      "signed to solve the long-term dependency problem in RNN. differentcroppingscales. TheUCM-captionsdatasetispro-\n",
      "The core of LSTM is three gates, including forgotten gate, vided by [6], which consists of 21 classes with 100 iamges\n",
      "inputgate,andoutputgate. LSTMfirstdecideswhichinfor- for each class. Each image is 256 × 256 pixels and the\n",
      "mationtodiscardthroughtheforgottengate. Thenbasedon resolution is 0.3048m. Based on the Sydney Data Set, the\n",
      "the input gate, it determines the values we are going to up- Sydney-captionsisalsoproposedin[6],whichcontains2329\n",
      "date. Meanwhile,thetanhlayerisusedtogeneratecandidate imageswith7classes. Forbothtwodatasets, fivesentences\n",
      "valuesthatcanbeaddedtothenetworkstate. Afterthat,the aregiventodescribeeachimage.\n",
      "outputlayercangetthefinaloutputstatebyfiltering. Inaddition,acommonlyusedmetricforimagecaptioning\n",
      "In the time t=1, the feature vector v is transferred to i.e.BLEU[12]isutilizedtoevaluatethequalityofthegener-\n",
      "0\n",
      "LSTM. When the t-th word is generated, we can represent atedcaptions.Asameasureofsimilaritybasedonaccuracy,it\n",
      "theprocessasfollows: isfirstproposedin[12]toevaluatethetaskofmachinetrans-\n",
      "lation. Indetail,itisabletoanalyzetheconsistencybetween\n",
      "s={w ,...,w ,...,w },t∈{0...N} (3)\n",
      "1 t N n-gramoccurrencesincandidateandreferencesentences.\n",
      "h =g(h ,v ,w ) (4)\n",
      "t t−1 0 t−1\n",
      "p =softmax(h ) (5) 3.2. Setup\n",
      "t t\n",
      "whereh representsthehiddenstateofLSTMattimet,and Based on the encoder-decoder framework, we train and test\n",
      "t\n",
      "w isthecorrespondingwordinthecaptions. Specially,g(·) our method with three CNN architectures in the encoding\n",
      "t\n",
      "denotestheprocessofLSTM.Aftergoingthroughasoftmax stage, including VGG-16, Inception-ResNetV2 and ResNet-\n",
      "fuction,wecangettheprobabilityofthenextwordappearing 152[13]. Inthedecodingprocess,anLSTMisusedtogen-\n",
      "i.e. p . Thefinalgoalofthisstepistominimizethenegative eratetheeffectivesentences. AsforLSTM,thedimensionof\n",
      "t\n",
      "likelihoodfunctionoftargetsentences,namelyLoss. wordembeddingandhiddenstatearesetto512and512,re-\n",
      "spectively. And the number of layers in LSTM is one layer.\n",
      "N\n",
      "(cid:88) WeuseStochasticGradientDescent(SGD)withthelearning\n",
      "Loss=− logp (w ) (6)\n",
      "t t\n",
      "rateof0.0001tooptimizethewholemodel.\n",
      "t=1\n",
      "10041\n",
      "Authorized licensed use limited to: University of Florida. Downloaded on March 27,2024 at 23:22:37 UTC from IEEE Xplore. Restrictions apply.\n",
      "classification,” IEEE Transactions on Geoscience and\n",
      "RemoteSensing,vol.57,no.2,pp.911–923,2019.\n",
      "[3] B. Qu, X. Li, D. Tao, and X. Lu, “Deep semantic un-\n",
      "derstanding of high resolution remote sensing image,”\n",
      "in International Conference on Computer, Information\n",
      "(a) A white air-(b) Lotsofhouses(c) The waves (d) Green plants\n",
      "planeisstoppedatarranged neatly slapping a white flourish on both andTelecommunicationSystems,2016,pp.124–128.\n",
      "theairport. and a road goes sand beach over banksoftheriver.\n",
      "throughthem. andoveragain. [4] O.Vinyals,A.Toshev,S.Bengio,andD.Erhan, “Show\n",
      "and tell: A neural image caption generator,” in IEEE\n",
      "Fig.2.Theresultsoftestimagesandcorespondinggenerated Conference on Computer Vision and Pattern Recogni-\n",
      "captions. tion,2015,pp.3156–3164.\n",
      "[5] S. Hochreiter and J. Schmidhuber, “Long short-term\n",
      "3.3. ResultsandAnalysis memory,” Neuralcomputation,vol.9,no.8,pp.1735–\n",
      "1780,1997.\n",
      "Table1andTable2showtheexperimentalresultsofthetwo\n",
      "datasetswithdifferentCNNarchitectures,respectively. Not- [6] B. Qu, X. Li, D. Tao, and X. Lu, “Deep semantic un-\n",
      "ingthats 1,s 2,s 3 respectivelydenotethescaleof1.0, 0.875 derstanding of high resolution remote sensing image,”\n",
      "and 0.66. From the results we can find that the evaluation in International Conference on Computer, Information\n",
      "scores on both UCM-captions and Sydney-captions with all andTelecommunicationSystems,2016,pp.124–128.\n",
      "the CNN architectures are getting higher, with the selected\n",
      "scales increasing. And the Resnet-152 is the best CNN ar- [7] Z. Shi and Z. Zou, “Can a machine generate human-\n",
      "chitectureforencodingprocessbetweenallthearchitectures likelanguagedescriptionsforaremotesensingimage?,”\n",
      "inexperiments. Theabundantexperimentalresultsprovethe IEEETransactionsonGeoscienceandRemoteSensing,\n",
      "effectiveness of the proposed Multi-Scale Cropping Mecha- vol.55,no.6,pp.3623–3634,2017.\n",
      "nism.\n",
      "[8] X.Lu,B.Wang,X.Zheng,andX.Li, “Exploringmod-\n",
      "els and data for remote sensing image caption genera-\n",
      "4. CONCLUSION\n",
      "tion,” IEEE Transactions on Geoscience and Remote\n",
      "Sensing,vol.56,no.4,pp.2183–2195,2018.\n",
      "Inthispaper, weproposeamulti-scalecroppingmechanism\n",
      "for training, which can extract advanced semantic features [9] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Ima-\n",
      "fromimagessothatitcangeneratemeaningfulsentencesfor genetclassificationwithdeepconvolutionalneuralnet-\n",
      "the task of remote sensing image captioning. Based on an works,” in International Conference on Neural Infor-\n",
      "popular encoder-decoder framework, three CNNs combined mationProcessingSystems,2012,pp.1097–1105.\n",
      "withLSTMarecomparedtovalidatethegeneralizationper-\n",
      "formanceofourmethod.Andtheexperimentalresultsontwo [10] K.He,X.Zhang,S.Ren,andJ.Sun, “Spatialpyramid\n",
      "datasetsshowtheeffectivenessoftheproposedmethod. poolingindeepconvolutionalnetworksforvisualrecog-\n",
      "nition,”IEEETransactionsonPatternAnalysisandMa-\n",
      "chineIntelligence,vol.37,no.9,pp.1904–1916,2015.\n",
      "5. ACKNOWLEDGMENT\n",
      "[11] J.Yu,C.Zhu,J.Zhang,Q.Huang,andD.Tao, “Spatial\n",
      "This work was supported by the National Natural Science\n",
      "pyramid-enhancedNetVLADwithandweightedtriplet\n",
      "Foundation of China under Grant U1864204 and 61773316,\n",
      "lossforplacerecognition,” IEEETransactionsonNeu-\n",
      "NaturalScienceFoundationofShaanxiProvinceunderGrant\n",
      "ralNetworksandLearningSystems,2019.\n",
      "2018KJXX-024,andProjectofSpecialZoneforNationalDe-\n",
      "fenseScienceandTechnologyInnovation. [12] K.Papineni,S.Roukos,T.Ward,andW.J.Zhu, “Bleu:\n",
      "A method for automatic evaluation of machine trans-\n",
      "lation,” in Association for Computational Linguistics,\n",
      "6. REFERENCES\n",
      "2002,pp.311–318.\n",
      "[1] Q.Wang,S.Liu,J.Chanussot,andX.Li,“Sceneclassi-\n",
      "[13] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual\n",
      "ficationwithrecurrentattentionofVHRremotesensing\n",
      "learning for image recognition,” in IEEE Conference\n",
      "images,”IEEETransactionsonGeoscienceandRemote\n",
      "onComputerVisionandPatternRecognition,2016,pp.\n",
      "Sensing,,no.99,pp.1–13,2018.\n",
      "770–778.\n",
      "[2] Q.Wang,X.He,andX.Li,“Localityandstructureregu-\n",
      "larizedlowrankrepresentationforhyperspectralimage\n",
      "10042\n",
      "Authorized licensed use limited to: University of Florida. Downloaded on March 27,2024 at 23:22:37 UTC from IEEE Xplore. Restrictions apply.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += page_text + \"\\n\"\n",
    "    return text\n",
    "\n",
    "pdf_path = \"1.pdf\" \n",
    "extracted_text = extract_text_from_pdf(pdf_path)\n",
    "print(extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35075f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks created: 4\n",
      "First chunk preview: MULTI-SCALECROPPINGMECHANISMFORREMOTESENSINGIMAGECAPTIONING XuetingZhang1,QiWang1,ShangdongChen2,XuelongLi1* 1SchoolofComputerScienceandCenterforOPTicalIMageryAnalysisandLearning(OPTIMAL), NorthwesternPolytechnicalUniversity,Xian710072,Shaanxi,P.R.China. 2SchoolofInformationScienceandTechnology, NorthwestUniversity,Xian710072,Shaanxi,P.R.China. ABSTRACT Imagecaptioning[3]isacomprehensivetaskwhichcom- binescomputervisionandnaturallanguageprocessing.Since Withtherapiddevelopmentofartificialsatelli\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)         # Replace multiple whitespace with a single space\n",
    "    text = re.sub(r'\\\\[a-z]+', '', text)       # Remove escape sequences like \\n, \\t, etc.\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)   # Remove non-ASCII characters\n",
    "    return text.strip()\n",
    "\n",
    "def split_into_chunks(text, chunk_size=450):\n",
    "    words = text.split()\n",
    "    return [\" \".join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "\n",
    "cleaned_text = clean_text(extracted_text)\n",
    "chunks = split_into_chunks(cleaned_text, chunk_size=450)\n",
    "\n",
    "print(\"Number of chunks created:\", len(chunks))\n",
    "print(\"First chunk preview:\", chunks[0][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ef08e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1489 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged Summary:\n",
      " in this paper , we propose a novel method for remote sensing image captioning . the method is based on a multi - modal neural network model , which is used to generate meaningful sentences . in this method , a feature map/vector is decoded into a sentence model with sev - convolutional neural network ( cNNs ) , while in decoder eral related datasets released . this method is applied to generate a large amount of ingimages . it is shown that the sydney-caption is a powerful tool for generating meaningful sentences for a given image . in this paper , we introduce a puter vision , extracting features from images by learning multi-scalecroppingtrainingmechanismtoimprovethegen-based method . based on the eigenvalues of the encoder , a set of encodings is derived , and a recursive method is developed . it is shown that , in the case of a fixed-length feature vector , it is possible to extract features from a given image by a multi - scalecropping mechanism . in this case , the recurrence - based method is used to extract the feature vector framework  in this paper , we present a new method for analyzing the performance of remote sensing images . the method is based on the encoding t denotestheprocessofLSTMattimet , which is used to controlthetransmissionofnetworkinformation . it is shown that the t -th word is derived from a symplectic , asymmetric , and asymmetry - based . we also show that , in the equivalence , the corresponding eigenvalues are derived . _ key words : _ _ @xmath0 . in this paper , we propose a method for generating a machine based on a multi - scalecroppingmechanism for remote sensing image captioning . the method is based upon a non - gaussian recurrence , which is characterized by a symmetries . we show that the method can be applied to a wide range of arbitrary scales , such as the eigenvalues of the corresponding machine , and the entanglement of the machine . in this work , it is shown that the algorithm can be used as a tool for obtaining a higher - order\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load the fine-tuned T5 model and tokenizer\n",
    "model_path = \"./t5_arxiv_full_model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "\n",
    "# Create the summarization pipeline\n",
    "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer, framework = \"pt\")\n",
    "\n",
    "def summarize_chunks(chunks):\n",
    "    summaries = []\n",
    "    for chunk in chunks:\n",
    "        # Prepend a prompt to guide the model for summarization\n",
    "        input_text = \"summarize: \" + chunk\n",
    "        try:\n",
    "            summary = summarizer(input_text, max_length=150, min_length=40, do_sample=False)[0][\"summary_text\"]\n",
    "        except Exception as e:\n",
    "            print(\"Error during summarization:\", e)\n",
    "            summary = \"\"\n",
    "        summaries.append(summary)\n",
    "    return \" \".join(summaries)\n",
    "\n",
    "\n",
    "merged_summary = summarize_chunks(chunks)\n",
    "print(\"Merged Summary:\\n\", merged_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d519fa7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Text Preview:\n",
      " multi scalecroppingmechanismforremotesensingimagecaptioning northwesternpolytechnicaluniversity northwestuniversity abstract withtherapiddevelopmentofartificialsatellite encoder decoder base method automaticallty ber high resolution remote sensing image easily learn hign level semantic feature dig textual obtain recently remote sense image captioning relationship dominate field image ing good perpormance encoder process aim tence remote sense image promote represent image feature map vector temp\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def text_processing(text: str) -> str:\n",
    "    doc = nlp(text.lower())\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and token.is_alpha]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "processed_text = text_processing(cleaned_text)\n",
    "print(\"Processed Text Preview:\\n\", processed_text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b76cd21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  MULTI-SCALECROPPINGMECHANISMFORREMOTESENSINGIM...   \n",
      "\n",
      "                                             summary  \\\n",
      "0  in this paper , we propose a novel method for ...   \n",
      "\n",
      "                                      processed_text  \n",
      "0  multi scalecroppingmechanismforremotesensingim...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_pdf(pdf_path):\n",
    "    # 1. Extract raw text from the PDF\n",
    "    raw_text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # 2. Clean the extracted text\n",
    "    cleaned_text = clean_text(raw_text)\n",
    "    \n",
    "    # 3. Split cleaned text into chunks\n",
    "    chunks = split_into_chunks(cleaned_text)\n",
    "    \n",
    "    # 4. Generate a merged summary for all chunks\n",
    "    merged_summary = summarize_chunks(chunks)\n",
    "    \n",
    "    # 5. Process the cleaned text using spaCy\n",
    "    processed_text = text_processing(cleaned_text)\n",
    "    \n",
    "    # Create a DataFrame to hold all results\n",
    "    data = {\n",
    "        \"text\": [cleaned_text],\n",
    "        \"summary\": [merged_summary],\n",
    "        \"processed_text\": [processed_text]\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "\n",
    "pdf_path = \"1.pdf\"\n",
    "df = process_pdf(pdf_path)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c48dedda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MULTI-SCALECROPPINGMECHANISMFORREMOTESENSINGIM...</td>\n",
       "      <td>in this paper , we propose a novel method for ...</td>\n",
       "      <td>multi scalecroppingmechanismforremotesensingim...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  MULTI-SCALECROPPINGMECHANISMFORREMOTESENSINGIM...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  in this paper , we propose a novel method for ...   \n",
       "\n",
       "                                      processed_text  \n",
       "0  multi scalecroppingmechanismforremotesensingim...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd96b85f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to final.csv\n"
     ]
    }
   ],
   "source": [
    "df.to_csv(\"final.csv\", index=False)\n",
    "print(\"Data saved to final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "477a972c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded:\n",
      "                                             summary\n",
      "0  in this paper , we propose a novel method for ...\n",
      "FAISS index built with 1 documents.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "# Load the CSV that contains the \"summary\" column\n",
    "df = pd.read_csv(\"final.csv\")\n",
    "print(\"Data loaded:\")\n",
    "print(df[['summary']].head())\n",
    "\n",
    "# Get the list of summaries\n",
    "summaries = df['summary'].tolist()\n",
    "\n",
    "# Load a SentenceTransformer model for embedding\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Generate embeddings for each summary\n",
    "embeddings = embedder.encode(summaries, convert_to_tensor=False)\n",
    "embeddings = np.array(embeddings).astype(\"float32\")\n",
    "\n",
    "# Build a FAISS index using L2 distance\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings)\n",
    "print(\"FAISS index built with\", index.ntotal, \"documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c02d1701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Summary Context:\n",
      " in this paper , we propose a novel method for remote sensing image captioning . the method is based on a multi - modal neural network model , which is used to generate meaningful sentences . in this method , a feature map/vector is decoded into a sentence model with sev - convolutional neural network ( cNNs ) , while in decoder eral related datasets released . this method is applied to generate a large amount of ingimages . it is shown that the sydney-caption is a powerful tool for generating meaningful sentences for a given image . in this paper , we introduce a puter vision , extracting features from images by learning multi-scalecroppingtrainingmechanismtoimprovethegen-based method . based on the eigenvalues of the encoder , a set of encodings is derived , and a recursive method is developed . it is shown that , in the case of a fixed-length feature vector , it is possible to extract features from a given image by a multi - scalecropping mechanism . in this case , the recurrence - based method is used to extract the feature vector framework  in this paper , we present a new method for analyzing the performance of remote sensing images . the method is based on the encoding t denotestheprocessofLSTMattimet , which is used to controlthetransmissionofnetworkinformation . it is shown that the t -th word is derived from a symplectic , asymmetric , and asymmetry - based . we also show that , in the equivalence , the corresponding eigenvalues are derived . _ key words : _ _ @xmath0 . in this paper , we propose a method for generating a machine based on a multi - scalecroppingmechanism for remote sensing image captioning . the method is based upon a non - gaussian recurrence , which is characterized by a symmetries . we show that the method can be applied to a wide range of arbitrary scales , such as the eigenvalues of the corresponding machine , and the entanglement of the machine . in this work , it is shown that the algorithm can be used as a tool for obtaining a higher - order in this paper , we propose a novel method for remote sensing image captioning . the method is based on a multi - modal neural network model , which is used to generate meaningful sentences . in this method , a feature map/vector is decoded into a sentence model with sev - convolutional neural network ( cNNs ) , while in decoder eral related datasets released . this method is applied to generate a large amount of ingimages . it is shown that the sydney-caption is a powerful tool for generating meaningful sentences for a given image . in this paper , we introduce a puter vision , extracting features from images by learning multi-scalecroppingtrainingmechanismtoimprovethegen-based method . based on the eigenvalues of the encoder , a set of encodings is derived , and a recursive method is developed . it is shown that , in the case of a fixed-length feature vector , it is possible to extract features from a given image by a multi - scalecropping mechanism . in this case , the recurrence - based method is used to extract the feature vector framework  in this paper , we present a new method for analyzing the performance of remote sensing images . the method is based on the encoding t denotestheprocessofLSTMattimet , which is used to controlthetransmissionofnetworkinformation . it is shown that the t -th word is derived from a symplectic , asymmetric , and asymmetry - based . we also show that , in the equivalence , the corresponding eigenvalues are derived . _ key words : _ _ @xmath0 . in this paper , we propose a method for generating a machine based on a multi - scalecroppingmechanism for remote sensing image captioning . the method is based upon a non - gaussian recurrence , which is characterized by a symmetries . we show that the method can be applied to a wide range of arbitrary scales , such as the eigenvalues of the corresponding machine , and the entanglement of the machine . in this work , it is shown that the algorithm can be used as a tool for obtaining a higher - order in this paper , we propose a novel method for remote sensing image captioning . the method is based on a multi - modal neural network model , which is used to generate meaningful sentences . in this method , a feature map/vector is decoded into a sentence model with sev - convolutional neural network ( cNNs ) , while in decoder eral related datasets released . this method is applied to generate a large amount of ingimages . it is shown that the sydney-caption is a powerful tool for generating meaningful sentences for a given image . in this paper , we introduce a puter vision , extracting features from images by learning multi-scalecroppingtrainingmechanismtoimprovethegen-based method . based on the eigenvalues of the encoder , a set of encodings is derived , and a recursive method is developed . it is shown that , in the case of a fixed-length feature vector , it is possible to extract features from a given image by a multi - scalecropping mechanism . in this case , the recurrence - based method is used to extract the feature vector framework  in this paper , we present a new method for analyzing the performance of remote sensing images . the method is based on the encoding t denotestheprocessofLSTMattimet , which is used to controlthetransmissionofnetworkinformation . it is shown that the t -th word is derived from a symplectic , asymmetric , and asymmetry - based . we also show that , in the equivalence , the corresponding eigenvalues are derived . _ key words : _ _ @xmath0 . in this paper , we propose a method for generating a machine based on a multi - scalecroppingmechanism for remote sensing image captioning . the method is based upon a non - gaussian recurrence , which is characterized by a symmetries . we show that the method can be applied to a wide range of arbitrary scales , such as the eigenvalues of the corresponding machine , and the entanglement of the machine . in this work , it is shown that the algorithm can be used as a tool for obtaining a higher - order\n"
     ]
    }
   ],
   "source": [
    "def retrieve_summary(query, k=3):\n",
    "    \"\"\"\n",
    "    Retrieves the top-k most relevant summaries based on the query.\n",
    "    \"\"\"\n",
    "    query_embedding = embedder.encode([query], convert_to_tensor=False)\n",
    "    query_embedding = np.array(query_embedding).astype(\"float32\")\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    retrieved = [summaries[i] for i in indices[0]]\n",
    "    return \" \".join(retrieved)\n",
    "\n",
    "# Test the retrieval function\n",
    "test_query = \"What are the main contributions of the research paper?\"\n",
    "print(\"Retrieved Summary Context:\\n\", retrieve_summary(test_query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39492ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key Loaded: True\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv() \n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(\"OpenAI API Key Loaded:\", bool(openai_api_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e216a282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2dfc63c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import tiktoken\n",
    "\n",
    "def truncate_prompt(prompt, max_tokens, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"\n",
    "    Truncates the prompt so that its token length does not exceed max_tokens.\n",
    "    \"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    tokens = encoding.encode(prompt)\n",
    "    if len(tokens) > max_tokens:\n",
    "        truncated_tokens = tokens[:max_tokens]\n",
    "        return encoding.decode(truncated_tokens)\n",
    "    return prompt\n",
    "\n",
    "def generate_openai_answer(prompt, model=\"gpt-3.5-turbo\", max_tokens=200, temperature=0.7, max_context_tokens=14000):\n",
    "    \"\"\"\n",
    "    Generates an answer using OpenAI's ChatCompletion API,\n",
    "    ensuring that the prompt does not exceed allowed token limits.\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): The full prompt.\n",
    "        model (str): The model to use.\n",
    "        max_tokens (int): Maximum new tokens to generate.\n",
    "        temperature (float): Sampling temperature.\n",
    "        max_context_tokens (int): Maximum allowed tokens for the prompt.\n",
    "        \n",
    "    Returns:\n",
    "        str: The generated answer.\n",
    "    \"\"\"\n",
    "    # Truncate the prompt if it exceeds our max_context_tokens limit.\n",
    "    prompt = truncate_prompt(prompt, max_context_tokens, model=model)\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers only based on the provided context.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message[\"content\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f043660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your query (or type 'exit' to quit): what is this paper about?\n",
      "Answer: This paper is about proposing a novel method for remote sensing image captioning using a multi-modal neural network model to generate meaningful sentences. The method involves decoding a feature map/vector into a sentence model with convolutional neural networks and applying a multi-scale cropping mechanism to improve feature extraction from images.\n",
      "Enter your query (or type 'exit' to quit): Ramaswamy contributions here?\n",
      "Answer: There is no mention of Ramaswamy or their contributions in the provided context.\n",
      "Enter your query (or type 'exit' to quit): Technology used?\n",
      "Answer: The technology used in the proposed method for remote sensing image captioning is a multi-modal neural network model, convolutional neural networks (CNNs), and a multi-scale cropping mechanism.\n",
      "Enter your query (or type 'exit' to quit): accuracy of cnn?\n",
      "Answer: The accuracy of the Convolutional Neural Networks (CNNs) is not explicitly mentioned in the provided context.\n",
      "Enter your query (or type 'exit' to quit): future scope?\n",
      "Answer: The future scope for the proposed method of remote sensing image captioning based on a multi-modal neural network model includes potential applications in various scales, further exploration into the eigenvalues of the machine, and advancements in utilizing the algorithm for higher-order tasks. The method's capabilities may expand to address a wider range of arbitrary scales and explore the entanglement of the machine, indicating promising opportunities for improvement and innovation in the field of image captioning.\n",
      "Enter your query (or type 'exit' to quit): exit\n"
     ]
    }
   ],
   "source": [
    "def answer_query(query):\n",
    "    # Retrieve the relevant summary context (using your retriever function)\n",
    "    context = retrieve_summary(query, k=3)\n",
    "    \n",
    "    # Construct a prompt that instructs the model to answer based only on the provided context\n",
    "    prompt = (\n",
    "        \"Use only the following context to answer the question. Do not add any information that is not present in the context. \"\n",
    "        \"Provide a complete, coherent answer in full sentences.\\n\\n\"\n",
    "        f\"Context: {context}\\n\\n\"\n",
    "        f\"Question: {query}\\n\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "    \n",
    "    # Generate an answer using your OpenAI generation function\n",
    "    answer = generate_openai_answer(prompt)\n",
    "    return answer\n",
    "\n",
    "# Test the integrated pipeline with an interactive loop:\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        user_query = input(\"Enter your query (or type 'exit' to quit): \")\n",
    "        if user_query.lower() == \"exit\":\n",
    "            break\n",
    "        print(\"Answer:\", answer_query(user_query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154fe654",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897e4340",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805edeec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
